{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb9e4601-01f1-4036-a3aa-34b1ac6a46d0",
   "metadata": {},
   "source": [
    "# Note\n",
    "\n",
    "This code is for demonstration of Newton's in `Python`. Please contact Luca Steyn to report any errors.\n",
    "\n",
    "# Imports\n",
    "\n",
    "The code depends only on `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4d2a07e-07a8-40a7-ab67-09a14df835e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feda8fd-3807-4745-b424-9ea2a10457cb",
   "metadata": {},
   "source": [
    "## Functions for Binary Logistic Regression with Newton's method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2a9544a-6351-442c-a4f2-ed572c7caa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid function to map logits to probabilities.\n",
    "    \"\"\"\n",
    "    return 1. / (1. + np.exp(-z))\n",
    "\n",
    "def cross_entropy_loss(X, y, beta):\n",
    "    \"\"\"\n",
    "    Compute the binary cross-entropy for logistic regression.\n",
    "    \"\"\"\n",
    "    z = X @ beta\n",
    "    p = sigmoid(z)\n",
    "    loss = -np.sum(y*np.log(p) + (1-y)*np.log(1-p))\n",
    "    return loss\n",
    "\n",
    "def _gradient(X, y, beta):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the binary cross-entropy for logistic regression.\n",
    "    \"\"\"\n",
    "    z = X @ beta\n",
    "    p = sigmoid(z)\n",
    "    return X.T @ (p - y)\n",
    "\n",
    "def _hessian(X, beta):\n",
    "    \"\"\"\n",
    "    Compute the Hessian matrix for logistic regression.\n",
    "    \"\"\"\n",
    "    z = X @ beta\n",
    "    p = sigmoid(z)\n",
    "    diag_entry = p*(1.-p)\n",
    "    diag_entry = diag_entry.ravel() # Note np.diag behaves differently for 1D (desired) and 2D arrays\n",
    "    U = np.diag(diag_entry)\n",
    "    return X.T @ U @ X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eda3288-68b7-4b64-867f-fe422c0e93b8",
   "metadata": {},
   "source": [
    "## Generate toy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec26090a-e372-4337-a078-c4ba8f0cef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated data\n",
    "np.random.seed(42)\n",
    "n, p = 5, 2\n",
    "X = np.random.randn(n, p)\n",
    "Xext = np.hstack((np.ones((n, 1)), X))  # Add a column of ones for the intercept\n",
    "true_beta = np.random.randn(p + 1).reshape(-1,1)\n",
    "y = (sigmoid(Xext @ true_beta + 0.5 * np.random.randn(n).reshape(-1,1)) > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb774fa3-29ca-4eaf-9b25-517b89d39f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af5b9e3a-8859-4d8c-a02b-b4bb3cbba81c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xext.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3bd6b47-d95d-4b63-90f3-086dfccb8977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8d27d5-15c2-4bb7-b9c9-eca3c05763f8",
   "metadata": {},
   "source": [
    "## Example of calculating loss, gradient and hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dea3317a-38a8-4260-ad7e-59b2b38c3af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4781507115540538"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_loss(Xext, y, true_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82d359ff-51fa-4078-b17c-8c8065948308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.86440716],\n",
       "       [0.99750596],\n",
       "       [0.39186338]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_gradient(Xext, y, true_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a430cf88-be65-492f-aed6-7b0ce111afab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.14434118, 0.40025429, 0.56484959],\n",
       "       [0.40025429, 0.7104337 , 0.40857727],\n",
       "       [0.56484959, 0.40857727, 0.76347483]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_hessian(Xext, true_beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106da025-e8b9-46be-842a-cec21d15fdb4",
   "metadata": {},
   "source": [
    "## Generate data for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "459ca314-a31f-4486-9f01-4e3cf2db700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated data\n",
    "np.random.seed(42)\n",
    "n, p = 100, 3\n",
    "X = np.random.randn(n, p)\n",
    "Xext = np.hstack((np.ones((n, 1)), X))  # Add a column of ones for the intercept\n",
    "true_beta = np.random.randn(p + 1).reshape(-1,1)\n",
    "y = (sigmoid(Xext @ true_beta + 0.5 * np.random.randn(n).reshape(-1,1)) > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cdc1d1-6cfa-4c9a-8028-57027007e0e3",
   "metadata": {},
   "source": [
    "## Code for Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "572f2565-cee5-4db1-9bc0-f28c931792c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_step(X, y, beta):\n",
    "    \"\"\"\n",
    "    Compute the Newton step and decrement.\n",
    "    \"\"\"\n",
    "    assert beta.shape[1] == 1, \"beta must be a column vector - array with shape (?,1).\"\n",
    "    assert y.shape[1] == 1, \"y must be a column vector - array with shape (?,1).\"\n",
    "    assert X.shape[1] == beta.shape[0], \"X and beta must have matching dimensions.\"\n",
    "    assert X.shape[0] == y.shape[0], \"X and y must have same length.\"\n",
    "    gradient = _gradient(X, y, beta)\n",
    "    hessian = _hessian(X, beta)\n",
    "    step = -1. * np.linalg.inv(hessian) @ gradient\n",
    "    decrement = -1. * gradient.T @ step\n",
    "    return step, gradient, decrement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "238bd0e0-8083-41d9-9eb4-894c44c7a6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.67041886],\n",
       "        [-0.44021255],\n",
       "        [ 0.59345487],\n",
       "        [ 0.77024027]]),\n",
       " array([[  8.23836731],\n",
       "        [  4.60434928],\n",
       "        [ -4.65907161],\n",
       "        [-10.79326082]]),\n",
       " array([[18.62840211]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newton_step(Xext, y, true_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2859056a-afbf-4bae-be30-6c2e10929942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking_line_search(X, y, beta, step, decrement, initial_step_size=1.0, alpha1=0.8, alpha2=0.5):\n",
    "    \"\"\"\n",
    "    Perform backtracking line search to find the step size using Newton's decrement.\n",
    "    \"\"\"\n",
    "    t = initial_step_size\n",
    "    loss_fn = cross_entropy_loss(X, y, beta)\n",
    "    # Note we add the step since it is already the correct direction and use decrement on RHS\n",
    "    while cross_entropy_loss(X, y, beta + t * step) > loss_fn - alpha2 * t * decrement:\n",
    "        t *= alpha1\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c004ecab-f56f-41c6-83c1-0f9bc417d5fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step, gradient, decrement = newton_step(Xext, y, true_beta)\n",
    "backtracking_line_search(Xext, y, true_beta, step, decrement, initial_step_size=1.0, alpha1=0.8, alpha2=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21d41837-7a98-4460-b2b2-65997a5e93cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_method_logistic_regression(X, y, num_iterations=100, tol=1e-4, alpha1=0.9, alpha2=0.3, random_state=37):\n",
    "    \"\"\"\n",
    "    Perform logistic regression using Newton's method with backtracking line search.\n",
    "    \"\"\"\n",
    "    n, p = X.shape\n",
    "    np.random.seed(random_state)\n",
    "    beta = np.random.normal(size=X.shape[1]).reshape(-1,1)\n",
    "    for i in range(num_iterations):\n",
    "        step, gradient, decrement = newton_step(X, y, beta)\n",
    "        \n",
    "        # Perform backtracking line search to find the step size\n",
    "        step_size = backtracking_line_search(X, y, beta, step, decrement, alpha1=alpha1, alpha2=alpha2)\n",
    "        \n",
    "        # Update the coefficients\n",
    "        beta += step_size * step\n",
    "        \n",
    "        # Print log-likelihood every 10 iterations for monitoring\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f\"Iteration {i + 1}: Loss {cross_entropy_loss(X, y, beta)}\")\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(gradient) < tol:\n",
    "            print(f\"Converged after {i + 1} iterations\")\n",
    "            break\n",
    "        \n",
    "        if (i+1) == num_iterations:\n",
    "            warnings.warn('Algorithm did not converge. Returning estimates of last update. Increase num_iterations or tol.')\n",
    "    \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81094690-6879-4dae-9aae-b8d7b0f49ac2",
   "metadata": {},
   "source": [
    "## Example on generated data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe99a64-a680-40db-a176-db7879e05f3f",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cd06b45-bb2c-4a17-b4de-dfeeec0d1700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10: Loss 9.828885730444613\n",
      "Converged after 10 iterations\n"
     ]
    }
   ],
   "source": [
    "beta_est = newton_method_logistic_regression(Xext, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43ccfd8-0bee-4f74-bc93-e39ca5c458a0",
   "metadata": {},
   "source": [
    "### Get training accuracy\n",
    "\n",
    "Note we do not consider overfitting nor generalization of the model here. It is purely to demonstrate the workings of Newton's method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b47642bb-f098-4aeb-9363-4ed847da2d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = sigmoid(Xext @ beta_est)\n",
    "preds = [1 if p>0.5 else 0 for p in probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "028cba10-a0f2-496b-b410-41a1457a7879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(preds == y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43e240c4-a8ab-4c67-b402-73cd0c34a43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96        77\n",
      "           1       0.87      0.87      0.87        23\n",
      "\n",
      "    accuracy                           0.94       100\n",
      "   macro avg       0.92      0.92      0.92       100\n",
      "weighted avg       0.94      0.94      0.94       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y.ravel(), preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e183c99-03b4-41c6-ba00-f4413562361a",
   "metadata": {},
   "source": [
    "#### E.g. where it doesn't converge because we do max 1 step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97b4b6d8-38b3-4db7-9fdf-1305d2bc860b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\AppData\\Local\\Temp\\ipykernel_10860\\3840427108.py:27: UserWarning: Algorithm did not converge. Returning estimates of last update. Increase num_iterations or tol.\n",
      "  warnings.warn('Algorithm did not converge. Returning estimates of last update. Increase num_iterations or tol.')\n"
     ]
    }
   ],
   "source": [
    "beta_est = newton_method_logistic_regression(Xext, y, num_iterations=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11113d72-2159-4d5c-86f4-975dd69ca13a",
   "metadata": {},
   "source": [
    "## Compare results with `sklearn`\n",
    "\n",
    "Note that `sklearn` uses more sophisticated optimization algorithms than the standard Newton's method with backtracking. It is optimized to be stable and efficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bca8c77a-e212-4a5c-8a93-70f2672af6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn\n",
    "model = LogisticRegression(penalty=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac8021b1-804a-4b73-bc9a-5bb7a7daa03e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(penalty=None)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(penalty=None)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(penalty=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64335c36-dc0c-4eb1-8977-f9135d37d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_sklearn = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e201e728-f3ba-4508-a4eb-ac9a1ca641f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(preds_sklearn == y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9783bef3-b8d8-4305-a5ab-31cca297697f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sklearn</th>\n",
       "      <th>custom</th>\n",
       "      <th>abs_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-6.857030</td>\n",
       "      <td>-1.071223</td>\n",
       "      <td>5.785808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-5.807819</td>\n",
       "      <td>-1.775366</td>\n",
       "      <td>4.032453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.365563</td>\n",
       "      <td>0.240600</td>\n",
       "      <td>6.124963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.616918</td>\n",
       "      <td>2.584335</td>\n",
       "      <td>4.032583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sklearn    custom  abs_diff\n",
       "0 -6.857030 -1.071223  5.785808\n",
       "1 -5.807819 -1.775366  4.032453\n",
       "2  6.365563  0.240600  6.124963\n",
       "3  6.616918  2.584335  4.032583"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = pd.DataFrame({\n",
    "    'sklearn': np.concatenate((model.intercept_, model.coef_.ravel())),\n",
    "    'custom': beta_est.ravel()\n",
    "})\n",
    "df_results['abs_diff'] = np.abs(df_results['sklearn'] - df_results['custom'])\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "114b1c9e-a918-47b9-8507-241b7ba35ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.82899501],\n",
       "       [-0.56018104],\n",
       "       [ 0.74729361],\n",
       "       [ 0.61037027]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc9ac21-f8ab-4bdc-9156-9617c2f6819c",
   "metadata": {},
   "source": [
    "## Extreme cases in the Loss\n",
    "\n",
    "Note that the cross-entropy is not defined if $p$ is zero or one. As $p$ becomes close to zero or one, the cross-entropy has a term converging to $\\log(0)$ which diverges to $-\\infty$. One approach is to add a small constant in the $\\log(\\cdot)$ part of the cross-entropy loss. Furthermore, note that perfect separation of the classes introduces further challenges. (See [this link](https://stats.stackexchange.com/questions/451119/proving-mles-undefined-for-logistic-regression-with-separable-classes-in-p-dime) where the following was found)\n",
    "\n",
    "### Perfect Separation in Logistic Regression\n",
    "\n",
    "If the classes are linearly separable, there exists a hyperplane that can completely distinguish between the two classes (_i.e._, all positive examples are on one side of the hyperplane and all negative examples are on the other). Mathematically, this means there exists a vector $\\boldsymbol{v}$ such that:\n",
    "\n",
    "- $\\boldsymbol{x}_i^{\\top}\\boldsymbol{v} > 0$ when $y_i = 1$, and\n",
    "- $\\boldsymbol{x}_i^{\\top}\\boldsymbol{v} < 0$ when $y_i = 0$.\n",
    "\n",
    "This implies that we can find a margin $a > 0$ such that:\n",
    "\n",
    "- $\\boldsymbol{x}_i^{\\top}\\boldsymbol{v} \\geq a$ when $y_i = 1$, and\n",
    "- $\\boldsymbol{x}_i^{\\top}\\boldsymbol{v} \\leq -a$ when $y_i = 0$.\n",
    "\n",
    "In this case, the logistic regression model can keep increasing the magnitude of $\\boldsymbol{v}$ (scaling it by a factor $\\xi$) to make the predicted probabilities $p_i = \\sigma(\\boldsymbol{x}_i^{\\top} \\boldsymbol{\\beta})$ for the positive class closer to 1 and for the negative class closer to 0. As $\\xi \\to \\infty$, the predicted probabilities $p_i$ for $y_i = 1$ approach 1, and $p_i$ for $y_i = 0$ approach 0.\n",
    "\n",
    "The likelihood function for the logistic regression model, given the observations $\\mathrm{X}$ and responses $\\boldsymbol{y}$, is:\n",
    "\n",
    "$$\n",
    "\\mathscr{L}(\\boldsymbol{\\beta}) = \\prod_{i : y_i = 1} \\sigma(\\boldsymbol{x}_i^{\\top} \\boldsymbol{\\beta}) \\prod_{i : y_i = 0} (1 - \\sigma(\\boldsymbol{x}_i^{\\top} \\boldsymbol{\\beta})),\n",
    "$$\n",
    "\n",
    "where $\\sigma(x) = \\frac{1}{1 + e^{-x}}$.\n",
    "\n",
    "For perfect separation:\n",
    "\n",
    "- As $\\xi \\to \\infty$, $\\boldsymbol{x}_i^{\\top} \\boldsymbol{\\beta} \\to \\infty$ for $y_i = 1$ leading to $\\sigma(x_i^{\\top} \\beta) \\to 1$.\n",
    "- Similarly, $\\boldsymbol{x}_i^{\\top} \\boldsymbol{\\beta} \\to -\\infty$ for $y_i = 0$ leading to $1 - \\sigma(x_i^{\\top} \\beta) \\to 1$.\n",
    "\n",
    "Thus, the likelihood approaches 1 as $\\xi \\to \\infty$. Therefore, the maximum likelihood estimate does not exist in the conventional sense; instead, the coefficients diverge to infinity.\n",
    "\n",
    "Given the perfect separation, there is no unique MLE because:\n",
    "\n",
    "- Any direction $\\boldsymbol{v}$ that correctly separates the data can be scaled to arbitrarily large magnitudes, leading to infinite possible solutions where the likelihood approaches its maximum value of 1.\n",
    "- Therefore, some coefficients $\\boldsymbol{\\beta}$ may diverge to infinity, while others may remain finite or become zero depending on their contribution to the separation.\n",
    "\n",
    "#### Effect on Hessian\n",
    "\n",
    "- When the model becomes extremely confident (_i.e._, $p_i$ is very close to 1 or 0), the term $p_i(1 - p_i)$ in the diagonal entries of $\\mathrm{U}$ in the Hessian becomes very close to zero.\n",
    "- This causes the Hessian matrix to approach singularity, making it numerically unstable to compute the inverse or use in Newton's method.\n",
    "\n",
    "If $p_i \\approx 1$ or $p_i \\approx 0$, then $p_i(1 - p_i) \\approx 0$, causing the Hessian to have near-zero diagonal elements.\n",
    "\n",
    "There are a few ways to address these challenges, like:\n",
    "\n",
    "- **Regularization:** Adding a small regularization term (like L2 regularization) to the loss function can help keep the Hessian from becoming singular by adding a positive constant to the diagonal.\n",
    "- **Alternative Algorithms:** Use alternative optimization algorithms that do not rely on the Hessian inversion, like gradient descent or quasi-Newton methods (_e.g._, BFGS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fbd21a0c-5d96-4669-89c8-bf1ace0c414b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated data with higher dimension and perfect separation\n",
    "np.random.seed(42)\n",
    "n, p = 100, 10\n",
    "X = np.random.randn(n, p)\n",
    "Xext = np.hstack((np.ones((n, 1)), X))  # Add a column of ones for the intercept\n",
    "true_beta = np.random.randn(p + 1).reshape(-1,1)\n",
    "y = (sigmoid(Xext @ true_beta + 0.5 * np.random.randn(n).reshape(-1,1)) > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0d13fe1-974d-4218-9397-94650e9d2d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10: Loss nan\n",
      "Converged after 16 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\AppData\\Local\\Temp\\ipykernel_10860\\4204095915.py:13: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -np.sum(y*np.log(p) + (1-y)*np.log(1-p))\n",
      "C:\\Users\\lucas\\AppData\\Local\\Temp\\ipykernel_10860\\4204095915.py:13: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -np.sum(y*np.log(p) + (1-y)*np.log(1-p))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_est = newton_method_logistic_regression(Xext, y)\n",
    "\n",
    "probs = sigmoid(Xext @ beta_est)\n",
    "preds = [1 if p>0.5 else 0 for p in probs]\n",
    "\n",
    "np.mean(preds == y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab247564-4102-4c36-8c21-399ae512ed24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        30\n",
      "           1       1.00      1.00      1.00        70\n",
      "\n",
      "    accuracy                           1.00       100\n",
      "   macro avg       1.00      1.00      1.00       100\n",
      "weighted avg       1.00      1.00      1.00       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y.ravel(), preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8367a7-966d-4dd5-9b7a-949273280b87",
   "metadata": {},
   "source": [
    "### Adding small constant to the cross-entropy\n",
    "\n",
    "Note this does not deal with the optimization problem. It only prevents the loss from becoming `nan`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8db23b6d-cca2-4138-a765-59bdb9ac3c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(X, y, beta, eps=1e-08):\n",
    "    \"\"\"\n",
    "    Compute the binary cross-entropy for logistic regression.\n",
    "    \"\"\"\n",
    "    z = X @ beta\n",
    "    p = sigmoid(z)\n",
    "    loss = -np.sum(y*np.log(p + eps) + (1-y)*np.log(1-p + eps))\n",
    "    return loss\n",
    "\n",
    "def _gradient(X, y, beta):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the binary cross-entropy for logistic regression.\n",
    "    \"\"\"\n",
    "    z = X @ beta\n",
    "    p = sigmoid(z)\n",
    "    return X.T @ (p - y)\n",
    "\n",
    "def _hessian(X, beta):\n",
    "    \"\"\"\n",
    "    Compute the Hessian matrix for logistic regression.\n",
    "    \"\"\"\n",
    "    z = X @ beta\n",
    "    p = sigmoid(z)\n",
    "    diag_entry = p*(1.-p)\n",
    "    diag_entry = diag_entry.ravel()\n",
    "    U = np.diag(diag_entry)\n",
    "    return X.T @ U @ X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "909e685e-9e29-485a-bc24-2ccc597ad8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10: Loss 0.03997938727492323\n",
      "Converged after 16 iterations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_est = newton_method_logistic_regression(Xext, y)\n",
    "\n",
    "probs = sigmoid(Xext @ beta_est)\n",
    "preds = [1 if p>0.5 else 0 for p in probs]\n",
    "\n",
    "np.mean(preds == y.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc205397-1723-414e-8544-20cb49345c8d",
   "metadata": {},
   "source": [
    "Now we see the loss is defined because we avoid the risk of $\\log(0)$. Note that $p$ and $1-p$ are always between zero and one. So $p+\\epsilon$ and $(1-p)+\\epsilon$ are always postive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c71f27-b20a-4905-ba31-f3efd5a653e5",
   "metadata": {},
   "source": [
    "## Instability due to class separation\n",
    "\n",
    "The separating hyperplane is not unique when the classes are easily linearly separable. For example, different initial values will give different solutions. One way to deal with this problem is through regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c1e7e5e-0be3-4b33-9ad0-ddef4ea6b5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10: Loss 0.09204935918079218\n",
      "Converged after 16 iterations\n",
      "For seed 1, the estimates are:\n",
      "[ 58.359  20.091   4.395 -15.333  30.707  29.773  39.516  30.235  53.845\n",
      " -24.835  36.248]\n",
      "And the accuracy is: 100.00%\n",
      "\n",
      "\n",
      "Iteration 10: Loss 0.09716928003398165\n",
      "Converged after 17 iterations\n",
      "For seed 11, the estimates are:\n",
      "[ 63.431  21.477   4.244 -16.607  34.297  32.744  42.36   32.94   58.509\n",
      " -27.534  38.528]\n",
      "And the accuracy is: 100.00%\n",
      "\n",
      "\n",
      "Iteration 10: Loss 0.12023484081627421\n",
      "Converged after 17 iterations\n",
      "For seed 111, the estimates are:\n",
      "[ 62.463  21.569   4.949 -16.324  32.434  31.847  42.492  32.427  57.588\n",
      " -26.446  38.865]\n",
      "And the accuracy is: 100.00%\n",
      "\n",
      "\n",
      "Iteration 10: Loss 0.21046852198287502\n",
      "Converged after 17 iterations\n",
      "For seed 1111, the estimates are:\n",
      "[ 59.269  20.399   4.442 -15.568  31.233  30.265  40.099  30.707  54.665\n",
      " -25.256  36.804]\n",
      "And the accuracy is: 100.00%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seeds = [1, 11, 111, 1111]\n",
    "\n",
    "for seed in seeds:\n",
    "    beta_est = newton_method_logistic_regression(Xext, y, random_state=seed)\n",
    "    probs = sigmoid(Xext @ beta_est)\n",
    "    preds = [1 if p>0.5 else 0 for p in probs]\n",
    "    acc = np.mean(preds == y.ravel())\n",
    "    print(f'For seed {seed}, the estimates are:\\n{np.round(beta_est.flatten(),3)}\\nAnd the accuracy is: {acc:.2%}\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7812b8-816d-43e9-b7d5-e14e844bc3b4",
   "metadata": {},
   "source": [
    "### Add L2 penalty\n",
    "\n",
    "Note we can add the L2 norm as a penalty to improve the instability. Remark that the code is set up to include an intercept, so we need to remove it from the vector and add a zero in the place of the intercept to avoid also regularizing it. Alternatively, the model can be fitted to the data without an intercept and the intercept can be estimated afterwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b98590fd-a2c4-492d-9e68-b4fc223ede00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def get_regularization_term(beta, fit_intercept):\n",
    "    if fit_intercept:\n",
    "        reg_term = np.r_[[[0]], beta[1:,:]]  # Regularize only coefficients, not intercept\n",
    "    else:\n",
    "        reg_term = beta\n",
    "    return reg_term\n",
    "\n",
    "def cross_entropy_loss(X, y, beta, lambda_, fit_intercept=True, eps=1e-04):\n",
    "    \"\"\"\n",
    "    Compute the binary cross-entropy for logistic regression with L2 regularization.\n",
    "    \"\"\"\n",
    "    z = X @ beta\n",
    "    p = sigmoid(z)\n",
    "    reg_term = get_regularization_term(beta, fit_intercept)\n",
    "    loss = -np.sum(y * np.log(p + eps) + (1. - y) * np.log(1. - p + eps)) + (lambda_ / 2.) * np.linalg.norm(reg_term)**2\n",
    "    return loss\n",
    "\n",
    "def _gradient(X, y, beta, lambda_, fit_intercept=True):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the binary cross-entropy for logistic regression with L2 regularization.\n",
    "    \"\"\"\n",
    "    z = X @ beta\n",
    "    p = sigmoid(z)\n",
    "    reg_term = get_regularization_term(beta, fit_intercept)\n",
    "    grad_ce = X.T @ (p - y)\n",
    "    full_grad = grad_ce + lambda_ * reg_term\n",
    "    return full_grad, grad_ce\n",
    "\n",
    "def _hessian(X, beta, lambda_, fit_intercept=True):\n",
    "    \"\"\"\n",
    "    Compute the Hessian matrix for logistic regression with L2 regularization.\n",
    "    \"\"\"\n",
    "    z = X @ beta\n",
    "    p = sigmoid(z)\n",
    "    diag_entry = p * (1. - p)\n",
    "    U = np.diag(diag_entry.ravel())\n",
    "    reg_term = np.eye(beta.shape[0])\n",
    "    if fit_intercept:\n",
    "        reg_term[0, 0] = 0.  # Exclude the intercept (beta[0]) from regularization\n",
    "    return X.T @ U @ X + lambda_ * reg_term\n",
    "\n",
    "def newton_step(X, y, beta, lambda_, fit_intercept=True):\n",
    "    \"\"\"\n",
    "    Compute the Newton step and decrement.\n",
    "    \"\"\"\n",
    "    assert beta.shape[1] == 1, \"beta must be a column vector - array with shape (?,1).\"\n",
    "    assert y.shape[1] == 1, \"y must be a column vector - array with shape (?,1).\"\n",
    "    assert X.shape[1] == beta.shape[0], \"X and beta must have matching dimensions.\"\n",
    "    assert X.shape[0] == y.shape[0], \"X and y must have the same length.\"\n",
    "    \n",
    "    gradient, grad_ce = _gradient(X, y, beta, lambda_, fit_intercept)\n",
    "    hessian = _hessian(X, beta, lambda_, fit_intercept)\n",
    "    \n",
    "    # Use np.linalg.solve for stability instead of np.linalg.inv\n",
    "    step = -np.linalg.solve(hessian, gradient)\n",
    "    decrement = gradient.T @ np.linalg.solve(hessian, gradient)\n",
    "    return step, gradient, decrement, grad_ce\n",
    "\n",
    "def backtracking_line_search(X, y, beta, lambda_, fit_intercept, step, decrement, initial_step_size=1.0, alpha1=0.8, alpha2=0.5):\n",
    "    \"\"\"\n",
    "    Perform backtracking line search to find the step size using Newton's decrement.\n",
    "    \"\"\"\n",
    "    t = initial_step_size\n",
    "    loss_fn = cross_entropy_loss(X, y, beta, lambda_, fit_intercept)\n",
    "    \n",
    "    while cross_entropy_loss(X, y, beta + t * step, lambda_, fit_intercept) > loss_fn - alpha2 * t * decrement:\n",
    "        t *= alpha1\n",
    "    return t\n",
    "\n",
    "def newton_method_logistic_regression(X, y, lambda_, fit_intercept=True, num_iterations=500, tol=1e-4, alpha1=0.8, alpha2=0.3, random_state=1):\n",
    "    \"\"\"\n",
    "    Perform logistic regression using Newton's method with backtracking line search.\n",
    "    \"\"\"\n",
    "    n, p = X.shape\n",
    "    np.random.seed(random_state)\n",
    "    beta = np.random.normal(size=(X.shape[1], 1), scale=0.5)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        step, gradient, decrement, grad_ce = newton_step(X, y, beta, lambda_, fit_intercept)\n",
    "        \n",
    "        # Perform backtracking line search to find the step size\n",
    "        step_size = backtracking_line_search(X, y, beta, lambda_, fit_intercept, step, decrement, alpha1=alpha1, alpha2=alpha2)\n",
    "        # Update the coefficients\n",
    "        beta += step_size * step\n",
    "        \n",
    "        # Print loss every 10 iterations for monitoring\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f\"Iteration {i + 1}: Loss {cross_entropy_loss(X, y, beta, lambda_, fit_intercept)}\")\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(gradient) < tol or decrement < tol:\n",
    "            print(f\"Converged after {i + 1} iterations\")\n",
    "            break\n",
    "        \n",
    "        if (i+1) == num_iterations:\n",
    "            warnings.warn('Algorithm did not converge. Returning estimates of last update. Increase num_iterations or tol.')\n",
    "    \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0099dc09-049a-4d2f-af91-2104df128284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data before applying regularization\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67dd76cc-a41f-40f5-8a89-7c411b6b128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(X)\n",
    "Xs = np.c_[np.ones(Xs.shape[0]), Xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "18c8d119-d6b2-4c0e-93b6-2dd45d3f667c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 8 iterations\n"
     ]
    }
   ],
   "source": [
    "beta_est = newton_method_logistic_regression(Xs, y, lambda_=0.1, fit_intercept=True, random_state=37, tol=1e-04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "78c2368f-2137-40ba-b618-40a606f9e1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 8 iterations\n",
      "For seed 1, the estimates are:\n",
      "[ 5.242  1.865  0.129 -1.374  2.6    2.456  3.531  2.136  3.583 -1.855\n",
      "  3.686]\n",
      "And the accuracy is: 100.00%\n",
      "\n",
      "\n",
      "Converged after 8 iterations\n",
      "For seed 11, the estimates are:\n",
      "[ 5.242  1.865  0.129 -1.374  2.6    2.457  3.532  2.136  3.584 -1.855\n",
      "  3.686]\n",
      "And the accuracy is: 100.00%\n",
      "\n",
      "\n",
      "Converged after 7 iterations\n",
      "For seed 111, the estimates are:\n",
      "[ 5.242  1.865  0.129 -1.374  2.6    2.456  3.531  2.136  3.584 -1.855\n",
      "  3.686]\n",
      "And the accuracy is: 100.00%\n",
      "\n",
      "\n",
      "Converged after 8 iterations\n",
      "For seed 1111, the estimates are:\n",
      "[ 5.242  1.865  0.129 -1.374  2.6    2.456  3.532  2.136  3.584 -1.855\n",
      "  3.686]\n",
      "And the accuracy is: 100.00%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seeds = [1, 11, 111, 1111]\n",
    "\n",
    "for seed in seeds:\n",
    "    beta_est = newton_method_logistic_regression(Xs, y, lambda_=0.1, fit_intercept=True, random_state=seed, tol=1e-03)\n",
    "    probs = sigmoid(Xext @ beta_est)\n",
    "    preds = [1 if p>0.5 else 0 for p in probs]\n",
    "    acc = np.mean(preds == y.ravel())\n",
    "    print(f'For seed {seed}, the estimates are:\\n{np.round(beta_est.flatten(),3)}\\nAnd the accuracy is: {acc:.2%}\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6f9fda-e295-4301-8d71-396a528b1704",
   "metadata": {},
   "source": [
    "Note how stable the predictions are now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "44d46abe-34d0-4c31-90ff-a788618212fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare to sklearn\n",
    "model = LogisticRegression(penalty='l2', C=1/0.1)\n",
    "\n",
    "model.fit(Xs[:,1:], y.ravel())\n",
    "\n",
    "preds_sklearn = model.predict(Xs[:,1:])\n",
    "\n",
    "np.mean(preds_sklearn == y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6c0cdd2e-56aa-4497-90dc-729d36ad68b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.2393375])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5e092e4f-73ed-4ea7-b357-de349f9efad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.86510055,  0.13062529, -1.37356496,  2.59694215,  2.45552131,\n",
       "         3.53049662,  2.13600648,  3.58198536, -1.85296703,  3.68519357]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7575cb14-2f59-4dd4-b82b-e8242466e150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sklearn</th>\n",
       "      <th>custom</th>\n",
       "      <th>abs_diff</th>\n",
       "      <th>true_coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.239337</td>\n",
       "      <td>5.241867</td>\n",
       "      <td>0.002529</td>\n",
       "      <td>1.399355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.865101</td>\n",
       "      <td>1.865037</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.924634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.130625</td>\n",
       "      <td>0.129173</td>\n",
       "      <td>0.001452</td>\n",
       "      <td>0.059630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.373565</td>\n",
       "      <td>-1.373893</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>-0.646937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.596942</td>\n",
       "      <td>2.600014</td>\n",
       "      <td>0.003072</td>\n",
       "      <td>0.698223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.455521</td>\n",
       "      <td>2.456486</td>\n",
       "      <td>0.000965</td>\n",
       "      <td>0.393485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.530497</td>\n",
       "      <td>3.531518</td>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.895193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.136006</td>\n",
       "      <td>2.136466</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>0.635172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.581985</td>\n",
       "      <td>3.583583</td>\n",
       "      <td>0.001597</td>\n",
       "      <td>1.049553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.852967</td>\n",
       "      <td>-1.854871</td>\n",
       "      <td>0.001904</td>\n",
       "      <td>-0.535235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.685194</td>\n",
       "      <td>3.685817</td>\n",
       "      <td>0.000623</td>\n",
       "      <td>1.317394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sklearn    custom  abs_diff  true_coef\n",
       "0   5.239337  5.241867  0.002529   1.399355\n",
       "1   1.865101  1.865037  0.000063   0.924634\n",
       "2   0.130625  0.129173  0.001452   0.059630\n",
       "3  -1.373565 -1.373893  0.000328  -0.646937\n",
       "4   2.596942  2.600014  0.003072   0.698223\n",
       "5   2.455521  2.456486  0.000965   0.393485\n",
       "6   3.530497  3.531518  0.001022   0.895193\n",
       "7   2.136006  2.136466  0.000460   0.635172\n",
       "8   3.581985  3.583583  0.001597   1.049553\n",
       "9  -1.852967 -1.854871  0.001904  -0.535235\n",
       "10  3.685194  3.685817  0.000623   1.317394"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = pd.DataFrame({\n",
    "    'sklearn': np.concatenate((model.intercept_, model.coef_.ravel())),\n",
    "    'custom': beta_est.ravel()\n",
    "})\n",
    "df_results['abs_diff'] = np.abs(df_results['sklearn'] - df_results['custom'])\n",
    "df_results['true_coef'] = true_beta\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab41f227-056e-4a10-bf87-ebe52353c41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 8 iterations\n"
     ]
    }
   ],
   "source": [
    "beta_est = newton_method_logistic_regression(Xs, y, lambda_=0.1, fit_intercept=True, random_state=35, \n",
    "                                             tol=1e-06, num_iterations=500, alpha1=0.8, alpha2=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d535038a-8b87-4d10-9819-9179c59cc169",
   "metadata": {},
   "source": [
    "# Note:\n",
    "\n",
    "The random initialization and hyperparameters can still influence the convergence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fb826718-f4dc-4fc6-8056-c52463c65177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10: Loss 8.408888426542694\n",
      "Iteration 20: Loss 8.408888426542653\n",
      "Iteration 30: Loss 8.408888426542653\n",
      "Iteration 40: Loss 8.408888426542653\n",
      "Iteration 50: Loss 8.408888426542653\n",
      "Iteration 60: Loss 8.408888426542653\n",
      "Iteration 70: Loss 8.408888426542653\n",
      "Iteration 80: Loss 8.408888426542653\n",
      "Iteration 90: Loss 8.408888426542653\n",
      "Iteration 100: Loss 8.408888426542653\n",
      "Iteration 110: Loss 8.408888426542653\n",
      "Iteration 120: Loss 8.408888426542653\n",
      "Iteration 130: Loss 8.408888426542653\n",
      "Iteration 140: Loss 8.408888426542653\n",
      "Iteration 150: Loss 8.408888426542653\n",
      "Iteration 160: Loss 8.408888426542653\n",
      "Iteration 170: Loss 8.408888426542653\n",
      "Iteration 180: Loss 8.408888426542653\n",
      "Iteration 190: Loss 8.408888426542653\n",
      "Iteration 200: Loss 8.408888426542653\n",
      "Iteration 210: Loss 8.408888426542653\n",
      "Iteration 220: Loss 8.408888426542653\n",
      "Iteration 230: Loss 8.408888426542653\n",
      "Iteration 240: Loss 8.408888426542653\n",
      "Iteration 250: Loss 8.408888426542653\n",
      "Iteration 260: Loss 8.408888426542653\n",
      "Iteration 270: Loss 8.408888426542653\n",
      "Iteration 280: Loss 8.408888426542653\n",
      "Iteration 290: Loss 8.408888426542653\n",
      "Iteration 300: Loss 8.408888426542653\n",
      "Iteration 310: Loss 8.408888426542653\n",
      "Iteration 320: Loss 8.408888426542653\n",
      "Iteration 330: Loss 8.408888426542653\n",
      "Iteration 340: Loss 8.408888426542653\n",
      "Iteration 350: Loss 8.408888426542653\n",
      "Iteration 360: Loss 8.408888426542653\n",
      "Iteration 370: Loss 8.408888426542653\n",
      "Iteration 380: Loss 8.408888426542653\n",
      "Iteration 390: Loss 8.408888426542653\n",
      "Iteration 400: Loss 8.408888426542653\n",
      "Iteration 410: Loss 8.408888426542653\n",
      "Iteration 420: Loss 8.408888426542653\n",
      "Iteration 430: Loss 8.408888426542653\n",
      "Iteration 440: Loss 8.408888426542653\n",
      "Iteration 450: Loss 8.408888426542653\n",
      "Iteration 460: Loss 8.408888426542653\n",
      "Iteration 470: Loss 8.408888426542653\n",
      "Iteration 480: Loss 8.408888426542653\n",
      "Iteration 490: Loss 8.408888426542653\n",
      "Iteration 500: Loss 8.408888426542653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\AppData\\Local\\Temp\\ipykernel_10860\\2234850326.py:99: UserWarning: Algorithm did not converge. Returning estimates of last update. Increase num_iterations or tol.\n",
      "  warnings.warn('Algorithm did not converge. Returning estimates of last update. Increase num_iterations or tol.')\n"
     ]
    }
   ],
   "source": [
    "beta_est = newton_method_logistic_regression(Xs, y, lambda_=0.1, fit_intercept=True, random_state=35, \n",
    "                                             tol=1e-08, num_iterations=500, alpha1=0.8, alpha2=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f1d1e71-93cf-4827-84cf-78a3952e6e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.24161014],\n",
       "       [ 1.8649807 ],\n",
       "       [ 0.12914944],\n",
       "       [-1.37387484],\n",
       "       [ 2.59993072],\n",
       "       [ 2.45637048],\n",
       "       [ 3.53137494],\n",
       "       [ 2.13631836],\n",
       "       [ 3.5833902 ],\n",
       "       [-1.85480143],\n",
       "       [ 3.68573051]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4295f92-0c99-4511-a05d-733af87ba225",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
